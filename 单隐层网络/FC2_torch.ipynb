{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b5ef56-2910-4f67-a658-5ceb37a2dd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:0\n",
      "device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "from datareader import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "# 支持函数\n",
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition\n",
    "\n",
    "def cross_entrophy(yhat, y, label_smooth=0.1, weight_decay=0):\n",
    "    # yhat是二维向量，第一个维度是batch， y是单维度的labels\n",
    "    # 有待进一步测试\n",
    "    real_y = torch.zeros(yhat.shape, device=device)\n",
    "    real_y += label_smooth/yhat.shape[1]   # 应该能广播吧\n",
    "    real_y[range(len(yhat)), y] += 1 - label_smooth\n",
    "    loss = -torch.mul(real_y, torch.log(yhat))\n",
    "    #除以batchsize\n",
    "    return loss.sum(1, keepdim=True) / len(y)\n",
    "\n",
    "\n",
    "def l2_penalty(w):\n",
    "    # l2惩罚函数，把这个加入损失函数内就行了\n",
    "    return torch.sum(w.pow(2))/2\n",
    "\n",
    "def dropout_layer_fun(X, p):\n",
    "    # dropout function 兼容 batch\n",
    "    assert 0 <= p <= 1\n",
    "    if p == 1:\n",
    "        return torch.zeros_like(X, device=device)\n",
    "    if p == 0:\n",
    "        return X\n",
    "    mask = (torch.rand(X.shape, device=device) > p).float()\n",
    "    return mask * X / (1.0 - p)\n",
    "\n",
    "#X = torch.arange(16, dtype=torch.float32).reshape(2, 8)\n",
    "#print(X)\n",
    "#print(dropout_layer_fun(X, 0.0))\n",
    "#print(dropout_layer_fun(X, 0.2))\n",
    "#print(dropout_layer_fun(X, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1538ba5-c160-45f2-b744-9292dc6e71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新参数\n",
    "def sgd(params, lr):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad\n",
    "            param.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2043f268-0e97-4475-ab7b-0fec9394c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型的支持函数\n",
    "def accuracy(yhat, y):\n",
    "    # 计算正确的***数量***\n",
    "    if len(yhat.shape) > 1 and yhat.shape[1] > 1:\n",
    "        yhat = yhat.argmax(axis=1)\n",
    "    cmp = yhat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy(net, data_iter, lossfun):\n",
    "    # 三个参数都是function\n",
    "    with torch.no_grad():\n",
    "        if isinstance(net, torch.nn.Module):\n",
    "            net.eval()\n",
    "        metric = [0.0, 0.0]\n",
    "        total_loss = 0\n",
    "        for X, y in data_iter:\n",
    "            yhat = net(X)\n",
    "            metric[0] += accuracy(yhat, y)\n",
    "            total_loss += lossfun(yhat=yhat, y=y).sum()\n",
    "            metric[1] += y.shape[0]\n",
    "        # 正确率和平均损失\n",
    "        return metric[0]/metric[1], total_loss/metric[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e39120-7d3c-4589-a683-432823e4bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_epoch_ch3(net, train_iter, loss, updater, dropout=None, dropout_p=0):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    metric = [0.0, 0.0, 0.0]\n",
    "    for X, y in train_iter:\n",
    "        #print(X.shape)\n",
    "        #print(y.shape)\n",
    "        yhat = net(X)\n",
    "        l = loss(yhat=yhat, y=y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backword()\n",
    "            updater.step()\n",
    "            metric[0] += accuracy(yhat, y)\n",
    "            metric[1] += y.shape[0]\n",
    "            metric[2] += float(l) * len(y)\n",
    "        else:\n",
    "            l.sum().backward()\n",
    "            updater()\n",
    "            metric[0] += accuracy(yhat, y)\n",
    "            metric[1] += y.shape[0]\n",
    "            metric[2] += float(l.sum())\n",
    "    # 正确率和平均损失\n",
    "    return metric[0]/metric[1], metric[2]/metric[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffad82d-dedb-4a2e-8baf-bff172640fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练主模块\n",
    "\n",
    "# hyperparameter\n",
    "lr = 0.02\n",
    "num_epoch = 75\n",
    "batch_size = 400\n",
    "dropout_p = 0.45\n",
    "\n",
    "inputlayer = 784\n",
    "hiddenlayer = 256\n",
    "hiddenlayer1 = 256\n",
    "hiddenlayer2 = 128\n",
    "outputlayer = 10\n",
    "\n",
    "# 初始化参数\n",
    "# Omega1 = torch.normal(0, 0.01, (inputlayer, hiddenlayer), requires_grad=True, device=device)\n",
    "# Theta1 = torch.zeros((1, hiddenlayer), requires_grad=True, device=device)\n",
    "# Omega2 = torch.normal(0, 0.01, (hiddenlayer, outputlayer), requires_grad=True, device=device)\n",
    "# Theta2 = torch.zeros((1, outputlayer), requires_grad=True, device=device)\n",
    "\n",
    "#print(Omega2)\n",
    "def myUpdater():\n",
    "    return sgd([Omega1, Theta1, Omega2, Theta2], lr)\n",
    "\n",
    "# 定义模型\n",
    "def FC2Layer_model(x, dropout=None, p=0):\n",
    "    x = torch.matmul(x, Omega1) - Theta1\n",
    "    x = torch.nn.functional.relu(x)\n",
    "    if dropout != None:\n",
    "        x = dropout(x, p)\n",
    "    x = torch.matmul(x, Omega2) - Theta2\n",
    "    x = torch.nn.functional.sigmoid(x)\n",
    "    return softmax(x) #用自己的softmax看看\n",
    "\n",
    "\n",
    "class FC3Layer_model(torch.nn.Module):\n",
    "    dropout1 = 0.25\n",
    "    dropout2 = 0.5\n",
    "    def __init__(self, inputlayer, hiddenlayer1, hiddenlayer2, outputlayer, is_training=True, dropoutF=None):\n",
    "        super(FC3Layer_model, self).__init__()\n",
    "        self.inputlayer = inputlayer\n",
    "        self.outputlayer = outputlayer\n",
    "        self.is_training = is_training\n",
    "        self.dropout = dropoutF\n",
    "        self.dropout_p = dropout_p\n",
    "        self.lin1 = torch.nn.Linear(inputlayer, hiddenlayer1)\n",
    "        self.lin2 = torch.nn.Linear(hiddenlayer1, hiddenlayer2)\n",
    "        self.lin3 = torch.nn.Linear(hiddenlayer2, outputlayer)\n",
    "\n",
    "    def forward(self, H1):\n",
    "        H1 = torch.nn.functional.relu(self.lin1(H1.reshape(-1, self.inputlayer)))\n",
    "        if self.is_training == True and not (self.dropout is None):\n",
    "            H1 = self.dropout(H1, self.dropout1)\n",
    "        H1 = torch.nn.functional.relu(self.lin2(H1))\n",
    "        if self.is_training == True and not (self.dropout is None):\n",
    "            H1 = self.dropout(H1, self.dropout2)\n",
    "        H1 = torch.nn.functional.sigmoid(self.lin3(H1))\n",
    "        return softmax(H1)\n",
    "\n",
    "    def Updater(self):\n",
    "        return sgd([c.bias for c in self.children()] + [c.weight for c in self.children()], lr)\n",
    "        \n",
    "    __call__ = forward   #兼容前面的类型\n",
    "\n",
    "    def train(self):  #重写一下模块的码，怕出错\n",
    "        self.is_training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.is_training = False\n",
    "\n",
    "\n",
    "def UseXavier(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# t = FC3Layer_model(inputlayer, hiddenlayer1, hiddenlayer2, outputlayer, True, dropout_layer_fun)\n",
    "#print(*[i.weight for i in  t.children()])\n",
    "# 数据导入\n",
    "\n",
    "train_reader = MNISTReader(1)\n",
    "test_reader = MNISTReader(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa949c1-f4d6-4d93-b98c-54778a301577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1; accuracy: train = 0.28036666666666665, test = 0.6247; loss: train = 0.005369657146930695, test = 0.004947320558130741\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m data_l \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#if epoch == 70:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#   lr /= 1.8\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     train_accu, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_ch3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataIter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_entrophy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mupdater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUpdater\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_layer_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# 计算测试集\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     test_accu, teat_loss \u001b[38;5;241m=\u001b[39m evaluate_accuracy(net\u001b[38;5;241m=\u001b[39mlayer3, data_iter\u001b[38;5;241m=\u001b[39mtest_reader\u001b[38;5;241m.\u001b[39mdataIter(batch_size), lossfun\u001b[38;5;241m=\u001b[39mcross_entrophy)\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mtrain_epoch_ch3\u001b[1;34m(net, train_iter, loss, updater, dropout, dropout_p)\u001b[0m\n\u001b[0;32m      4\u001b[0m     net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m metric \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#print(X.shape)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#print(y.shape)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m net(X)\n\u001b[0;32m     10\u001b[0m     l \u001b[38;5;241m=\u001b[39m loss(yhat\u001b[38;5;241m=\u001b[39myhat, y\u001b[38;5;241m=\u001b[39my)\n",
      "File \u001b[1;32m~\\0000\\MyNN\\单隐层网络\\datareader.py:100\u001b[0m, in \u001b[0;36mMNISTReader.dataIter\u001b[1;34m(self, BatchSize)\u001b[0m\n\u001b[0;32m     98\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch_indices:\n\u001b[1;32m--> 100\u001b[0m     re \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(re[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    102\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(re[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\0000\\MyNN\\单隐层网络\\datareader.py:70\u001b[0m, in \u001b[0;36mMNISTReader.get_pic\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m     66\u001b[0m label_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_file_metadata_bytes \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m784\u001b[39m):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# 直接在最初的读取的时候就将数据全部归一化\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_pic\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28;43mint\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_bytes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_start\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m     71\u001b[0m     img_start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_bytes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_bytes[label_start: label_start\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbig\u001b[39m\u001b[38;5;124m'\u001b[39m, signed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer3 = FC3Layer_model(inputlayer, hiddenlayer1, hiddenlayer2, outputlayer, True, dropout_layer_fun)\n",
    "layer3.apply(UseXavier)\n",
    "layer3.to(device) #将模型复制到gpu\n",
    "data_l = []\n",
    "for epoch in range(num_epoch):\n",
    "    #if epoch == 70:\n",
    "    #   lr /= 1.8\n",
    "    train_accu, train_loss = train_epoch_ch3(net=layer3,\n",
    "                                             train_iter=train_reader.dataIter(batch_size),\n",
    "                                             loss=cross_entrophy,\n",
    "                                             updater=layer3.Updater,\n",
    "                                             dropout=dropout_layer_fun, dropout_p=dropout_p)\n",
    "\n",
    "    # 计算测试集\n",
    "    test_accu, teat_loss = evaluate_accuracy(net=layer3, data_iter=test_reader.dataIter(batch_size), lossfun=cross_entrophy)\n",
    "    data_l.append((train_accu, train_loss, test_accu, teat_loss))\n",
    "    print(f'epoch: {epoch+1}; accuracy: train = {train_accu}, test = {test_accu}; loss: train = {train_loss}, test = {teat_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c99457-698b-430e-a5f8-bf1d32d37237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#绘图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_l = [i for i in range(1, num_epoch+1)]\n",
    "y1 = [x[0] for x in data_l]\n",
    "y2 = [x[2] for x in data_l]\n",
    "\n",
    "y3 = [x[1] for x in data_l]\n",
    "y4 = [float(x[3]) for x in data_l]\n",
    "\n",
    "plt.figure(figsize=(18, 6), dpi=100)\n",
    "plt.rc(\"font\", family='MicroSoft YaHei', weight=\"bold\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('正确率')\n",
    "plt.plot(epoch_l, y1, color='red', linestyle=':', label='train set')\n",
    "plt.plot(epoch_l, y2, color='green', linestyle='-.', label='test set')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(epoch_l, y3, color='red', linestyle=':', label='train set')\n",
    "plt.plot(epoch_l, y4, color='green', linestyle='-.', label='test set')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23aba6-563c-4bc7-8ae5-e6a72ba6bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计分标签的正确率\n",
    "if isinstance(layer3, torch.nn.Module):\n",
    "    layer3.eval()\n",
    "metric = [[0 for i in range(10)] for i in range(10)]  # 行表示正确的标签，列表示错误的标签\n",
    "false_label = []\n",
    "for X, y in test_reader.dataIter(1):\n",
    "    yhat = layer3(X)\n",
    "    yhat = yhat.argmax(axis=1)\n",
    "    metric[int(y)][int(yhat)] += 1\n",
    "# 正确率和平均损失\n",
    "for i in range(10):\n",
    "    print(f'数字{i}的正确率{round(metric[i][i]/sum(metric[i]) * 100, 2)}%', end='\\t')\n",
    "    metric[i][i] = 0\n",
    "    j = metric[i].index(max(metric[i]))\n",
    "    print(f'最大误报数字为{j}, 次数为{metric[i][j]}')\n",
    "    #print(' '.join([f'数字{j}误报次数{metric[i][j]}; ' if (metric[i][j] > 0 and i != j) else '' for j in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d9f20-3879-4097-9ea2-98c8690a35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示效果\n",
    "import matplotlib.pyplot as plt\n",
    "from os import system\n",
    "for imdata, label in test_reader.dataIter(1):\n",
    "    system('cls')\n",
    "    predict_re = layer3(imdata)\n",
    "    imdata = imdata *128 + 128\n",
    "    imdata = imdata.reshape((28, 28))\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(imdata.cpu())\n",
    "    plt.show()\n",
    "    print(predict_re)\n",
    "    predict_num = torch.argmax(predict_re)\n",
    "    print(f'模型预测数字为: {predict_num}，正确答案为: {int(label)}，', end='')\n",
    "    input('输入任意值继续: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85c204-92a7-4770-ab02-4eec2de8f71a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
