{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecb40df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T12:44:02.282061Z",
     "start_time": "2023-09-14T12:44:02.258232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fini\n"
     ]
    }
   ],
   "source": [
    "from os import system\n",
    "import numpy as np\n",
    "\n",
    "# 创建数据结构\n",
    "InputLayerC = 784\n",
    "HiddenLayerC = 300\n",
    "OutputLayerC = 10\n",
    "\n",
    "Omega1 = np.random.normal(0, 0.05, (InputLayerC, HiddenLayerC)) # 缩小初始化的参数值，让最后一层的sigmoid激活函数的输入不至于太大，导致梯度消失\n",
    "Theta1 = np.random.normal(0, 0.05, (1, HiddenLayerC))\n",
    "\n",
    "Omega2 = np.random.normal(0, 0.05, (HiddenLayerC, OutputLayerC))\n",
    "Theta2 = np.random.normal(0, 0.05, (1, OutputLayerC))\n",
    "\n",
    "# 定义全局变量\n",
    "LearnRate1 = 0.05\n",
    "LearnRate2 = 0.07\n",
    "Alpha = 0.1       #平滑系数\n",
    "LAMBDA = 0.9      #咱也不知道取多少好的正则化参数\n",
    "augmentRateFalse = 1  #惩罚的学习率的修正系数（增加惩罚的学习率），救，起反作用了\n",
    "print('fini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909366b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T12:44:04.137521Z",
     "start_time": "2023-09-14T12:44:04.113558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fini\n"
     ]
    }
   ],
   "source": [
    "# 定义支持的数学函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + pow(np.e, -x))\n",
    "\n",
    "# Q1 这个迭代是in place的，但在当前工程中不存在问题，只是是一个隐患\n",
    "def ReLU(x):\n",
    "    # 输入的x是numpy的array\n",
    "    if type(x) == type(1):\n",
    "        return max(0, x)\n",
    "    else:\n",
    "        with np.nditer(x, op_flags=['readwrite']) as it:\n",
    "           for item in it:\n",
    "               item[...] = max(item, 0)\n",
    "        return x\n",
    "def delta_Relu(x):\n",
    "    if type(x) == type(1):\n",
    "        return 1 if x > 0 else 0\n",
    "    else:\n",
    "        with np.nditer(x, op_flags=['readwrite']) as it:\n",
    "           for item in it:\n",
    "               item[...] = 1 if item > 0 else 0\n",
    "        return x\n",
    "\n",
    "def softmax(Yhat):\n",
    "    exped = np.exp(Yhat)\n",
    "    x_sum = np.sum(exped, axis=1, keepdims=True)\n",
    "    return exped / x_sum\n",
    "\n",
    "def CrossEntropy(modelRe, y):\n",
    "    Y = np.array([[Alpha/OutputLayerC for i in range(OutputLayerC)]])\n",
    "    Y[0][y] += 1 - Alpha\n",
    "    return -np.sum(np.log2(modelRe) * Y)\n",
    "\n",
    "def BatchNormalization(data, scale=2.3, shift=0):\n",
    "    # a b 表示标准化的上界和下界\n",
    "    mean = np.mean(data)\n",
    "    std = (np.std(data) + 0.001)\n",
    "    return ((data - mean)/std)*scale + shift\n",
    "    \n",
    "print('fini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82aaf9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T12:44:06.166440Z",
     "start_time": "2023-09-14T12:44:06.048210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fini\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "from datareader import *\n",
    "Reader = MNISTReader(mode=1)\n",
    "TrainSetSize = Reader.get_pic_count()\n",
    "\n",
    "# 区分验证集和训练集\n",
    "TrainRate = 0.8\n",
    "from random import shuffle\n",
    "DataList = [i for i in range(1, TrainSetSize+1)]\n",
    "shuffle(DataList)\n",
    "A = int(TrainRate * len(DataList))\n",
    "TrainSet = DataList[:A]\n",
    "VerifySet = DataList[A:]\n",
    "print('fini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e66d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T12:44:13.069329Z",
     "start_time": "2023-09-14T12:44:13.045428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fini\n"
     ]
    }
   ],
   "source": [
    "def predict(pixels):\n",
    "    global Omega1, Omega2, Theta1, Theta2\n",
    "    x_l = pixels\n",
    "    x_l = np.array(x_l).reshape(1, InputLayerC)\n",
    "    #HiddenOut = ReLU(BatchNormalization(np.dot(x_l, Omega1) - Theta1))  # 较之上一个模型，隐含层的激活函数变成了ReLU\n",
    "    HiddenOut = ReLU(np.dot(x_l, Omega1) - Theta1)\n",
    "    YHat = sigmoid(np.dot(HiddenOut, Omega2) - Theta2)\n",
    "    #加入一个BatchNormalization\n",
    "    outputX = BatchNormalization(YHat)\n",
    "    return softmax(YHat)\n",
    "\n",
    "\n",
    "# train\n",
    "def train_StdBP_regulated(x, y, Falg=0):\n",
    "    #正则化的\n",
    "    # Falg = 0  是否print中间参数\n",
    "    global Omega1, Omega2, Theta1, Theta2\n",
    "    x_l = np.array(x).reshape(1, InputLayerC)\n",
    "    Hidden_ReLU_x = np.dot(x_l, Omega1) - Theta1\n",
    "    #HiddenOut = ReLU(BatchNormalization(Hidden_ReLU_x))\n",
    "    HiddenOut = ReLU(Hidden_ReLU_x)\n",
    "    outputX = np.dot(HiddenOut, Omega2) - Theta2  #输出层激活函数的x\n",
    "    #加入一个BatchNormalization\n",
    "    outputX = BatchNormalization(outputX)\n",
    "    YHat = sigmoid(outputX)\n",
    "    if(Falg): print('outputX: ', outputX)\n",
    "    if(Falg): print('YHat', YHat)\n",
    "    if(Falg): print(f'正确答案{y}')\n",
    "    yhat_softmax = softmax(YHat)\n",
    "    # 计算损失函数 在每一个yhat上的梯度                                                #！！！！！！！！这里可能有大bug\n",
    "    LF_Yhat = (yhat_softmax - Alpha/OutputLayerC) * augmentRateFalse\n",
    "    LF_Yhat[0][y] = LF_Yhat[0][y]/augmentRateFalse + Alpha - 1\n",
    "    if(Falg): print('LF_Yhat: ', LF_Yhat)\n",
    "    g_j = YHat * (1 - YHat) * LF_Yhat\n",
    "    if(Falg): print('g_j: ', g_j)\n",
    "\n",
    "    delta_w = LearnRate1 * g_j * HiddenOut.reshape(HiddenLayerC, 1)\n",
    "    #print('hidden_out: ', HiddenOut)\n",
    "    if(Falg): print('delta_w', delta_w)\n",
    "\n",
    "    #print(\"隐含层到输出层权重更新 delta_w:\")\n",
    "    #print(delta_w)\n",
    "    Omega2 = Omega2 - LAMBDA * delta_w - LearnRate1 * (2-2 * LAMBDA) * Omega2\n",
    "    #print(Omega2)\n",
    "    aaa = np.sum(np.absolute(delta_w))/(HiddenLayerC * OutputLayerC)\n",
    "    \n",
    "    Theta2 = Theta2 + LearnRate1 * g_j * LAMBDA - LearnRate1 * (2-2 * LAMBDA) * Theta2\n",
    "    #print(Theta2)\n",
    "    \n",
    "    delta_e = delta_Relu(HiddenOut) * np.dot(g_j, Omega2.reshape(OutputLayerC, HiddenLayerC))  # 也可能有bug！！！！！！！！！\n",
    "   \n",
    "    #delta_e = HiddenOut * (1 - HiddenOut) * np.dot(g_j, Omega2.reshape(OutputLayerC, HiddenLayerC))  # 试试也改成Softmax，但是记得也该前面的码\n",
    "    if(Falg): print('delta_e: ', delta_e)  #shape (1,9)\n",
    "\n",
    "    delta_v = LearnRate2 * delta_e * x_l.reshape(InputLayerC, 1)    # 这里可能存在过多delta v 为0的情况\n",
    "    Omega1 = Omega1 - LAMBDA * delta_v - LearnRate2 * (2-2 * LAMBDA) * Omega1\n",
    "    Theta1 = Theta1 + LearnRate2 * delta_e * LAMBDA - LearnRate2 * (2-2 * LAMBDA) * Theta1\n",
    "    \n",
    "    bbb = np.sum(np.absolute(delta_v))/(HiddenLayerC * InputLayerC)\n",
    "    return (aaa, bbb)  #先不写综合BP，但是返回参数修改的平均值\n",
    "    \n",
    "def train_StdBP(x, y, Falg=0):\n",
    "    #Falg = 0\n",
    "    global Omega1, Omega2, Theta1, Theta2\n",
    "    x_l = np.array(x).reshape(1, InputLayerC)\n",
    "    Hidden_ReLU_x = np.dot(x_l, Omega1) - Theta1\n",
    "    #HiddenOut = ReLU(BatchNormalization(Hidden_ReLU_x))\n",
    "    HiddenOut = ReLU(Hidden_ReLU_x)\n",
    "    outputX = np.dot(HiddenOut, Omega2) - Theta2  #输出层激活函数的x\n",
    "    #加入一个BatchNormalization\n",
    "    outputX = BatchNormalization(outputX)\n",
    "    YHat = sigmoid(outputX)\n",
    "    if(Falg): print('outputX: ', outputX)\n",
    "    if(Falg): print('YHat', YHat)\n",
    "    if(Falg): print(f'正确答案{y}')\n",
    "    yhat_softmax = softmax(YHat)\n",
    "    # 计算损失函数 在每一个yhat上的梯度                                                #！！！！！！！！这里可能有大bug\n",
    "    LF_Yhat = (yhat_softmax - Alpha/OutputLayerC) * augmentRateFalse\n",
    "    LF_Yhat[0][y] = LF_Yhat[0][y]/augmentRateFalse + Alpha - 1\n",
    "    if(Falg): print('LF_Yhat: ', LF_Yhat)\n",
    "    g_j = YHat * (1 - YHat) * LF_Yhat\n",
    "    if(Falg): print('g_j: ', g_j)\n",
    "\n",
    "    delta_w = LearnRate1 * g_j * HiddenOut.reshape(HiddenLayerC, 1)\n",
    "    #print('hidden_out: ', HiddenOut)\n",
    "    if(Falg): print('delta_w', delta_w)\n",
    "\n",
    "    #print(\"隐含层到输出层权重更新 delta_w:\")\n",
    "    #print(delta_w)\n",
    "    Omega2 = Omega2 - delta_w\n",
    "    #print(Omega2)\n",
    "    aaa = np.sum(np.absolute(delta_w))/(HiddenLayerC * OutputLayerC)\n",
    "    \n",
    "    Theta2 = Theta2 + LearnRate1 * g_j\n",
    "    #print(Theta2)\n",
    "    \n",
    "    delta_e = delta_Relu(HiddenOut) * np.dot(g_j, Omega2.reshape(OutputLayerC, HiddenLayerC))  # 也可能有bug！！！！！！！！！\n",
    "   \n",
    "    #delta_e = HiddenOut * (1 - HiddenOut) * np.dot(g_j, Omega2.reshape(OutputLayerC, HiddenLayerC))  # 试试也改成Softmax，但是记得也该前面的码\n",
    "    if(Falg): print('delta_e: ', delta_e)  #shape (1,9)\n",
    "\n",
    "    delta_v = LearnRate2 * delta_e * x_l.reshape(InputLayerC, 1)    # 这里可能存在过多delta v 为0的情况\n",
    "    Omega1 = Omega1 - delta_v\n",
    "    Theta1 = Theta1 + LearnRate2 * delta_e\n",
    "    \n",
    "    bbb = np.sum(np.absolute(delta_v))/(HiddenLayerC * InputLayerC)\n",
    "    \n",
    "    return (aaa, bbb)  #先不写综合BP，但是返回参数修改的平均值\n",
    "\n",
    "\n",
    "def train_CummBP(xinput):\n",
    "    \n",
    "    Falg = 0\n",
    "    global Omega1, Omega2, Theta1, Theta2, Reader\n",
    "    Size = len(xinput)\n",
    "    delta_v = np.zeros((InputLayerC, HiddenLayerC))\n",
    "    delta_e = np.zeros((1, HiddenLayerC))\n",
    "    delta_w = np.zeros((HiddenLayerC, OutputLayerC))\n",
    "    g_j = np.zeros((1, OutputLayerC))\n",
    "    \n",
    "    for i in range(Size):\n",
    "        pdata = Reader.get_pic(xinput[i])\n",
    "        x_l = np.array(pdata[0]).reshape(1, InputLayerC)\n",
    "        Hidden_ReLU_x = np.dot(x_l, Omega1) - Theta1\n",
    "        #HiddenOut = ReLU(BatchNormalization(Hidden_ReLU_x))\n",
    "        HiddenOut = ReLU(Hidden_ReLU_x)\n",
    "        outputX = np.dot(HiddenOut, Omega2) - Theta2  #输出层激活函数的x\n",
    "        #加入一个BatchNormalization\n",
    "        outputX = BatchNormalization(outputX)\n",
    "        YHat = sigmoid(outputX)\n",
    "        yhat_softmax = softmax(YHat)\n",
    "        # 计算损失函数 在每一个yhat上的梯度                                                #！！！！！！！！这里可能有大bug\n",
    "        LF_Yhat = (yhat_softmax - Alpha/OutputLayerC) * augmentRateFalse\n",
    "        LF_Yhat[0][pdata[1]] = LF_Yhat[0][pdata[1]]/augmentRateFalse + Alpha - 1   \n",
    "        g_j_local = YHat * (1 - YHat) * LF_Yhat\n",
    "        delta_w += LearnRate1 * g_j_local * HiddenOut.reshape(HiddenLayerC, 1)\n",
    "        g_j += g_j_local\n",
    "        delta_e_local = delta_Relu(HiddenOut) * np.dot(g_j_local, Omega2.reshape(OutputLayerC, HiddenLayerC))  # 也可能有bug！！！！！！！！！\n",
    "        delta_v += LearnRate2 * delta_e_local * x_l.reshape(InputLayerC, 1)    # 这里可能存在过多delta v 为0的情况\n",
    "        delta_e += delta_e_local\n",
    "\n",
    "    # 考虑是否正则化  \n",
    "    Omega2 = Omega2 - delta_w  * LAMBDA - LearnRate1 * (2-2 * LAMBDA) * Omega2\n",
    "    Theta2 = Theta2 + LearnRate1 * g_j  * LAMBDA - LearnRate1 * (2-2 * LAMBDA) * Theta2\n",
    "    Omega1 = Omega1 - delta_v  * LAMBDA - LearnRate2 * (2-2 * LAMBDA) * Omega1\n",
    "    Theta1 = Theta1 + LearnRate2 * delta_e  * LAMBDA - LearnRate2 * (2-2 * LAMBDA) * Theta1 \n",
    "    \n",
    "    aaa = np.sum(np.absolute(delta_w))/(HiddenLayerC * OutputLayerC)\n",
    "    bbb = np.sum(np.absolute(delta_v))/(HiddenLayerC * InputLayerC)\n",
    "    # 返回一下两层总梯度的变化\n",
    "    return (aaa, bbb)\n",
    "print('fini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a8cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集平均交叉熵为3.2893420978070584，错误率0.8729，w和v参数的平均修改量为0.005873309154178322和0.0007657324510367856，接下来第2遍迭代\n",
      "验证集平均交叉熵为3.2322152212938375，错误率0.8315，w和v参数的平均修改量为0.006500980588448594和0.0009028908199204016，接下来第3遍迭代\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m verify_l[:]:\n\u001b[0;32m     40\u001b[0m     pic \u001b[38;5;241m=\u001b[39m verify_Reader\u001b[38;5;241m.\u001b[39mget_pic(item)\n\u001b[1;32m---> 41\u001b[0m     modelre \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     var\u001b[38;5;241m.\u001b[39mappend(CrossEntropy(modelre, pic[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     43\u001b[0m     model_number \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(modelre)\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(pixels)\u001b[0m\n\u001b[0;32m      4\u001b[0m x_l \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_l)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, InputLayerC)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#HiddenOut = ReLU(BatchNormalization(np.dot(x_l, Omega1) - Theta1))  # 较之上一个模型，隐含层的激活函数变成了ReLU\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m HiddenOut \u001b[38;5;241m=\u001b[39m ReLU(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOmega1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m Theta1)\n\u001b[0;32m      7\u001b[0m YHat \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(HiddenOut, Omega2) \u001b[38;5;241m-\u001b[39m Theta2)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#加入一个BatchNormalization\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 尝试训练\n",
    "\n",
    "OutputOmega1 = np.zeros((InputLayerC, HiddenLayerC)) \n",
    "OutputTheta1 = np.zeros((1, HiddenLayerC))\n",
    "OutputOmega2 = np.zeros((HiddenLayerC, OutputLayerC))\n",
    "OutputTheta2 = np.zeros((1, OutputLayerC))\n",
    "OutputIter = 0\n",
    "\n",
    "verify_Reader = MNISTReader(2)\n",
    "verify_Len = verify_Reader.total_img\n",
    "verify_l = [i for i in range(1, verify_Len + 1)]\n",
    "iter = 1\n",
    "test_l = [(0, 100000, 100)]  #记录迭代次数，累积交叉熵和错误率\n",
    "lastFalse = 1\n",
    "while iter < 10000:\n",
    "    curchange = [0, 0]\n",
    "    shuffle(TrainSet)\n",
    "    \n",
    "    if iter%8 == 0:\n",
    "        LearnRate1 *= 0.5\n",
    "        LearnRate2 *= 0.5\n",
    "    if lastFalse <= 0.5:\n",
    "        # 标准BP\n",
    "        for i in range(100):\n",
    "            #print(f\"第{iter}遍遍历训练集， 第{i+1}条记录\")\n",
    "            pdata = Reader.get_pic(TrainSet[i])\n",
    "            record = train_StdBP_regulated(pdata[0], pdata[1])\n",
    "            curchange[0] += record[0]\n",
    "            curchange[1] += record[1]\n",
    "            #input(f'w和v参数的平均修改量为{record[0]}和{record[1]}，输入任意数值继续')'''\n",
    "    if lastFalse > 0.5:\n",
    "        # 累积BP训练\n",
    "        curchange = train_CummBP(TrainSet[:100])\n",
    "    \n",
    "    # 计算平均交叉熵\n",
    "    shuffle(verify_l)\n",
    "    var = []\n",
    "    wrong_count = 0\n",
    "    for item in verify_l[:]:\n",
    "        pic = verify_Reader.get_pic(item)\n",
    "        modelre = predict(pic[0])\n",
    "        var.append(CrossEntropy(modelre, pic[1]))\n",
    "        model_number = np.argmax(modelre)\n",
    "        if model_number != pic[1]:\n",
    "            wrong_count += 1\n",
    "\n",
    "    #print(var)\n",
    "    mean_var = sum(var)/len(var)\n",
    "    false_rate = wrong_count/len(var)\n",
    "    # 计算验证正确率\n",
    "    print(f\"验证集平均交叉熵为{mean_var}，错误率{false_rate}，w和v参数的平均修改量为{curchange[0]}和{curchange[1]}，接下来第{iter+1}遍迭代\")\n",
    "    if lastFalse > false_rate:\n",
    "        lastFalse = false_rate\n",
    "        OutputIter = iter\n",
    "        OutputOmega1 = np.array(Omega1)\n",
    "        OutputOmega2 = np.array(Omega2)\n",
    "        OutputTheta1 = np.array(Theta1)\n",
    "        OutputTheta2 = np.array(Theta2)\n",
    "    if iter - OutputIter > 9: #9次都没有更新更优秀，退出\n",
    "        break\n",
    "\n",
    "    test_l.append((iter, mean_var, false_rate))\n",
    "    iter += 1\n",
    "print(f'训练结束，最优情况错误率{lastFalse}')\n",
    "Omega1 = OutputOmega1\n",
    "Omega2 = OutputOmega2\n",
    "Theta1 = OutputTheta1\n",
    "Theta2 = OutputTheta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99798b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [x[0] for x in test_l[1:]]\n",
    "y1 = [x[1] for x in test_l[1:]]\n",
    "y2 = [x[2] for x in test_l[1:]]\n",
    "\n",
    "plt.figure(figsize=(40, 10), dpi=100)\n",
    "plt.rc(\"font\", family='MicroSoft YaHei', weight=\"bold\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('验证集交叉熵')\n",
    "plt.plot(x, y1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('错误率')\n",
    "plt.plot(x, y2)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d8603-295c-49f9-8bb7-e443640da34f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 中途test 模块\n",
    "'''\n",
    "verify_Reader = MNISTReader(2)\n",
    "verify_Len = verify_Reader.total_img\n",
    "verify_l = [i for i in range(1, verify_Len + 1)]\n",
    "iter = 1\n",
    "test_l = [(0, 100000, 100)]  #记录迭代次数，累积交叉熵和错误率\n",
    "lastFalse = 1\n",
    "while iter < 10000:\n",
    "    system('cls')\n",
    "    curchange = [0, 0]\n",
    "    shuffle(TrainSet)\n",
    "    # 标准BP\n",
    "    for i in range(100):\n",
    "        #print(f\"第{iter}遍遍历训练集， 第{i+1}条记录\")\n",
    "        pdata = Reader.get_pic(TrainSet[i])\n",
    "        record = train_StdBP(pdata[0], pdata[1], Falg=1)\n",
    "        curchange[0] += record[0]\n",
    "        curchange[1] += record[1]\n",
    "        input(f'w和v参数的平均修改量为{record[0]}和{record[1]}，输入任意数值继续')\n",
    "    \n",
    "    # 计算平均交叉熵\n",
    "    shuffle(verify_l)\n",
    "    var = []\n",
    "    wrong_count = 0\n",
    "    for item in verify_l[:]:\n",
    "        pic = verify_Reader.get_pic(item)\n",
    "        modelre = predict(pic[0])\n",
    "        var.append(CrossEntropy(modelre, pic[1]))\n",
    "        model_number = np.argmax(modelre)\n",
    "        if model_number != pic[1]:\n",
    "            wrong_count += 1\n",
    "\n",
    "    #print(var)\n",
    "    mean_var = sum(var)/len(var)\n",
    "    false_rate = wrong_count/len(var)\n",
    "    # 计算验证正确率\n",
    "    print(f\"验证集平均交叉熵为{mean_var}，错误率{false_rate}，w和v参数的平均修改量为{curchange[0]}和{curchange[1]}，按任意键第{iter+1}遍遍历数据\")\n",
    "    if lastFalse > false_rate:\n",
    "        lastFalse = false_rate\n",
    "    test_l.append((iter, mean_var, false_rate))\n",
    "    iter += 1\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bb653-9342-4b49-b323-873c0e4ac199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示效果\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "while True:\n",
    "    #system('cls')\n",
    "    seleted_p_id = random.randint(1, len(verify_l)-1)\n",
    "    pdata = verify_Reader.get_pic(seleted_p_id)  #不敢从训练集内获取图片，怕出意外\n",
    "    #pdata = Reader.get_pic(seleted_p_id)\n",
    "    imdata = np.array(pdata[0]) * 255\n",
    "    imdata = imdata.reshape((28, 28))\n",
    "    #print(imdata)\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(imdata)\n",
    "    plt.show()\n",
    "    predict_re = predict(pdata[0])\n",
    "    print(predict_re)\n",
    "    predict_num = np.argmax(predict_re)\n",
    "    print(f'模型预测数字为: {predict_num}，正确答案为: {pdata[1]}，',end='')\n",
    "    input('输入任意值继续: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e53a6-1158-4933-97c6-0122fee0ffb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.837158,
   "position": {
    "height": "144.097px",
    "left": "1103.02px",
    "right": "20px",
    "top": "126.972px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
